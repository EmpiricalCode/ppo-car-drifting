## Benchmarking

My custom PPO implementation (top) vs OpenAI's ppo_baseline (bottom). Notice that OpenAI's ppo_baseline reward means seem to plateau at ~400.

![PPO Custom Training](github/ppo_custom_training.png)
![PPO Baseline Training](github/ppo_baseline_training.png)

## PPO Baseline

Snippets of a PPO baseline algorithm running in the custom drifting environment after training.

![PPO Baseline Test 1](github/ppo_baseline_1.gif)
![PPO Baseline Test 2](github/ppo_baseline_2.gif)